{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84495c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langchain Simple Agent\n",
    "\n",
    "# Import required libraries\n",
    "\n",
    "# For calling the Openai API\n",
    "from langchain_openai import ChatOpenAI # LLM BRAIN - ENGINE\n",
    "# # For calling the Anthropic API\n",
    "# from langchain_anthropic import AnthropicChat\n",
    "# # For Azure OpenAI API\n",
    "# from langchain_azure_openai import AzureChatOpenAI\n",
    "# # For calling the Google API\n",
    "# from langchain_google import ChatGoogle\n",
    "# # For calling the Ollama API\n",
    "# from langchain_ollama import OllamaChat\n",
    "# # For calling the Cohere API\n",
    "# from langchain_cohere import CohereChat\n",
    "# # For calling the HuggingFace API\n",
    "# from langchain_huggingface import HuggingFaceChat\n",
    "# # For calling the Google Gemini API\n",
    "# from langchain_gemini import GeminiChat\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder # MESSAGES - CHASSIS\n",
    "# ChatPromptTemplate: It is used to create a prompt template for chat models. This helps in structuring the input for the model.\n",
    "# MessagesPlaceholder: It is used to create a placeholder for messages in the prompt template.\n",
    "\n",
    "from langchain.tools import tool # HANDS - DASHBOARD\n",
    "# tool: It is used to define a tool that can be called by the agent. This allows the agent to perform specific actions or retrieve information.\n",
    "\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "# create_tool_calling_agent: It is used to create an agent that can call tools.\n",
    "# AgentExecutor: It is used to execute the agent with the provided tools and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da670ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import PyPDFLoader # a class used to load text from PDF files\n",
    "\n",
    "# Imports a powerful text-splitting tool that breaks large documents into manageable chunks for embedding.\n",
    "# LLMs have context length limits, so splitting documents into chunks is essential.\n",
    "# Why recursive?\n",
    "# It tries to split at logical boundaries (like paragraphs, sentences, etc.) before falling back to raw character limits.\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# OpenAIEmbeddings - to convert text into vector format using OpenAI’s embedding API\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "from langchain.vectorstores import FAISS # a fast in-memory vector search engine used for semantic search.\n",
    "\n",
    "\"\"\"Imports RetrievalQA, a chain that connects:\n",
    "-A retriever (like FAISS)\n",
    "-A language model (like GPT): To answer questions using retrieved context.\n",
    "\n",
    "This is the heart of a RAG system — combining search + generation\"\"\"\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab39cf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load environment variables from a .env file and set up API keys and other configurations\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8462b98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 2: Load and preprocess the document\n",
    "pdf_path = \"docs/the_nestle_hr_policy_pdf_2012.pdf\"\n",
    "\n",
    "try:\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    documents = loader.load()\n",
    "    print(f\"Loaded {len(documents)} pages from the document.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading document: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ab161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 3: Split the document into smaller chunks for processing\n",
    "#RecursiveCharacter text splitter tries to split at logical boundaries (like paragraphs, sentences, etc.) before falling back to raw character limits.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # Maximum size of each chunk\n",
    "    chunk_overlap=100  # Overlap between chunks to maintain context\n",
    ")\n",
    "#chunk has text , vector representation and metadata\n",
    "#chuking is important because LLMs have context length limits\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Split document into {len(chunks)} chunks.\")\n",
    "print(chunks[0])  # Print the first chunk to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56d7fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4 : Create embeddings for the chunks using OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY) #text-embedding-ada-002\n",
    "\n",
    "print(embeddings.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6697a4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 5: Create a vector store using FAISS to store and retrieve embeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "print(\"Vector store created with FAISS.\")\n",
    "#save the vector store to disk\n",
    "#vector_store.save_local(\"faiss_index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3075775c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 6: Create a RetrievalQA chain that uses the vector store to answer questions\n",
    "llm = ChatOpenAI(model_name=\"gpt-4-turbo\", openai_api_key=OPENAI_API_KEY)\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "#available search types: similarity, mmr, hybrid\n",
    "\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, # LLM model\n",
    "    retriever=retriever,# Retriever to fetch relevant documents\n",
    "    return_source_documents=False # Whether to return source documents along with the answer\n",
    ")\n",
    "print(\"RAG chain created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dc782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question: str):\n",
    "    \"\"\"Function to answer a question using the RAG chain.\"\"\"\n",
    "    result = rag_chain.invoke({\"query\":  question})\n",
    "    answer = result['result']\n",
    "    #source_docs = result['source_documents']\n",
    "    \n",
    "    print(f\"Question: {question}\\n\")\n",
    "    print(f\"Answer: {answer}\\n\")\n",
    "    #print(\"Source Documents:\")\n",
    "    #for i, doc in enumerate(source_docs):\n",
    "     #   print(f\"Document {i+1}:\\n{doc.page_content}\\n\")\n",
    "    \n",
    "   # return answer, #source_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c5b227",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 7: Test the RAG system with a sample question\n",
    "sample_question = \"What are the key responsibilities of the HR department as mentioned in the document?\"\n",
    "answer_question(sample_question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b629e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ask another question\n",
    "another_question = \"What does SPIL stand for?\"\n",
    "answer_question(another_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648a1fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what are the working hours mentioned in the document?\"\n",
    "answer_question(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0581b81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question= \"My child secured 79.9% in 12th, what rewards can he get as per the document?\"\n",
    "answer_question(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf96d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"where can i take the international training courses?\"\n",
    "answer_question(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
